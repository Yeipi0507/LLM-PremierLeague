#!/usr/bin/env python3
"""
Performance Testing Suite - LLM Premier League
Compara rendimiento entre modo Claude AI (ON) vs modo Local (OFF)
"""

import requests
import time
import json
import statistics
from datetime import datetime
from typing import Dict, List, Tuple
import os
import sys

# ConfiguraciÃ³n
API_BASE_URL = "http://localhost:8080/api"
TEST_ITERATIONS = 5  # NÃºmero de iteraciones por test
TIMEOUT = 60  # Timeout en segundos

# Test cases predefinidos
PREDICTION_TESTS = [
    {"home_team": "Liverpool", "away_team": "Chelsea"},
    {"home_team": "Arsenal", "away_team": "Man City"},
    {"home_team": "Man United", "away_team": "Tottenham"},
    {"home_team": "Newcastle", "away_team": "Aston Villa"},
    {"home_team": "Brighton", "away_team": "West Ham"}
]

ANALYSIS_TESTS = [
    {"team": "Liverpool"},
    {"team": "Chelsea"}, 
    {"team": "Arsenal"},
    {"team": "Man City"},
    {"team": "Man United"}
]

CHAT_TESTS = [
    {"message": "Â¿QuiÃ©n serÃ¡ el mÃ¡ximo goleador esta temporada?"},
    {"message": "Â¿CÃ³mo va el Arsenal esta temporada?"},
    {"message": "Â¿QuiÃ©n ganarÃ­a entre Liverpool y Chelsea?"},
    {"message": "Â¿CuÃ¡les son los favoritos al tÃ­tulo?"},
    {"message": "Â¿QuÃ© tal los fichajes del Manchester United?"}
]

class PerformanceTester:
    def __init__(self):
        self.results = {
            'claude_ai_on': {},
            'claude_ai_off': {},
            'timestamp': datetime.now().isoformat(),
            'test_config': {
                'iterations': TEST_ITERATIONS,
                'timeout': TIMEOUT,
                'base_url': API_BASE_URL
            }
        }
    
    def check_server_status(self) -> bool:
        """Verificar que el servidor estÃ© funcionando"""
        try:
            response = requests.get(f"{API_BASE_URL}/health", timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def toggle_ai_mode(self, enable: bool) -> bool:
        """Cambiar el modo AI del sistema"""
        try:
            payload = {"use_claude_ai": enable}
            response = requests.post(
                f"{API_BASE_URL}/toggle-ai", 
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                print(f"âœ… Modo cambiado a: {'Claude AI' if enable else 'Local'}")
                return data.get('current_mode') == enable
            else:
                print(f"âŒ Error cambiando modo: {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ Error en toggle: {e}")
            return False
    
    def measure_endpoint_performance(self, endpoint: str, method: str, payload: Dict = None) -> Dict:
        """Medir rendimiento de un endpoint especÃ­fico"""
        times = []
        success_count = 0
        responses = []
        
        for i in range(TEST_ITERATIONS):
            try:
                start_time = time.time()
                
                if method == 'GET':
                    response = requests.get(f"{API_BASE_URL}/{endpoint}", timeout=TIMEOUT)
                elif method == 'POST':
                    response = requests.post(f"{API_BASE_URL}/{endpoint}", json=payload, timeout=TIMEOUT)
                
                end_time = time.time()
                
                if response.status_code == 200:
                    success_count += 1
                    response_time = end_time - start_time
                    times.append(response_time)
                    responses.append(response.json())
                    print(f"  âœ… IteraciÃ³n {i+1}: {response_time:.3f}s")
                else:
                    print(f"  âŒ IteraciÃ³n {i+1}: Error {response.status_code}")
                    
            except requests.Timeout:
                print(f"  â±ï¸ IteraciÃ³n {i+1}: Timeout ({TIMEOUT}s)")
            except Exception as e:
                print(f"  âŒ IteraciÃ³n {i+1}: Error - {e}")
        
        if times:
            return {
                'success_rate': success_count / TEST_ITERATIONS,
                'avg_time': statistics.mean(times),
                'min_time': min(times),
                'max_time': max(times),
                'median_time': statistics.median(times),
                'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
                'total_tests': TEST_ITERATIONS,
                'successful_tests': success_count,
                'sample_responses': responses[:2]  # Primeras 2 respuestas como muestra
            }
        else:
            return {
                'success_rate': 0,
                'avg_time': 0,
                'min_time': 0,
                'max_time': 0,
                'median_time': 0,
                'std_dev': 0,
                'total_tests': TEST_ITERATIONS,
                'successful_tests': 0,
                'sample_responses': []
            }
    
    def test_predictions(self) -> Dict:
        """Test de predicciones de partidos"""
        print("ğŸ“Š Testing Predictions...")
        results = {}
        
        for i, test_case in enumerate(PREDICTION_TESTS):
            print(f"  ğŸˆ Test {i+1}: {test_case['home_team']} vs {test_case['away_team']}")
            results[f"prediction_{i+1}"] = self.measure_endpoint_performance(
                "predict", "POST", test_case
            )
        
        return results
    
    def test_analysis(self) -> Dict:
        """Test de anÃ¡lisis de equipos"""
        print("ğŸ“‹ Testing Team Analysis...")
        results = {}
        
        for i, test_case in enumerate(ANALYSIS_TESTS):
            print(f"  ğŸ€ Test {i+1}: {test_case['team']}")
            results[f"analysis_{i+1}"] = self.measure_endpoint_performance(
                "analyze", "POST", test_case
            )
        
        return results
    
    def test_chat(self) -> Dict:
        """Test de chat conversacional"""
        print("ğŸ’¬ Testing Chat...")
        results = {}
        
        for i, test_case in enumerate(CHAT_TESTS):
            print(f"  ğŸ’­ Test {i+1}: {test_case['message'][:50]}...")
            results[f"chat_{i+1}"] = self.measure_endpoint_performance(
                "chat", "POST", test_case
            )
        
        return results
    
    def test_basic_endpoints(self) -> Dict:
        """Test de endpoints bÃ¡sicos"""
        print("ğŸ”§ Testing Basic Endpoints...")
        results = {}
        
        endpoints = [
            ("health", "GET"),
            ("teams", "GET"),
            ("stats", "GET"),
            ("system", "GET")
        ]
        
        for endpoint, method in endpoints:
            print(f"  ğŸŒ Testing /{endpoint}")
            results[endpoint] = self.measure_endpoint_performance(endpoint, method)
        
        return results
    
    def run_full_test_suite(self, mode: str):
        """Ejecutar suite completa de tests"""
        print(f"\nğŸš€ Ejecutando tests en modo: {mode}")
        print("=" * 60)
        
        mode_results = {}
        
        # Tests bÃ¡sicos (mÃ¡s rÃ¡pidos)
        mode_results['basic'] = self.test_basic_endpoints()
        
        # Tests de predicciones
        mode_results['predictions'] = self.test_predictions()
        
        # Tests de anÃ¡lisis
        mode_results['analysis'] = self.test_analysis()
        
        # Tests de chat
        mode_results['chat'] = self.test_chat()
        
        return mode_results
    
    def calculate_summary_stats(self, mode_results: Dict) -> Dict:
        """Calcular estadÃ­sticas resumidas"""
        all_times = []
        all_success_rates = []
        
        for category in mode_results.values():
            for test_result in category.values():
                if test_result['success_rate'] > 0:
                    all_times.append(test_result['avg_time'])
                all_success_rates.append(test_result['success_rate'])
        
        if all_times:
            return {
                'overall_avg_time': statistics.mean(all_times),
                'overall_success_rate': statistics.mean(all_success_rates),
                'fastest_response': min(all_times),
                'slowest_response': max(all_times),
                'total_tests_run': sum(len(cat) * TEST_ITERATIONS for cat in mode_results.values())
            }
        else:
            return {
                'overall_avg_time': 0,
                'overall_success_rate': 0,
                'fastest_response': 0,
                'slowest_response': 0,
                'total_tests_run': 0
            }
    
    def run_comparative_tests(self):
        """Ejecutar tests comparativos completos"""
        print("ğŸ† LLM PREMIER LEAGUE - PERFORMANCE TESTING SUITE")
        print("=" * 80)
        
        if not self.check_server_status():
            print("âŒ ERROR: Servidor no disponible en", API_BASE_URL)
            return False
        
        print("âœ… Servidor conectado correctamente")
        
        # Test 1: Modo Claude AI OFF (Local)
        print("\n" + "="*80)
        print("ğŸ”„ FASE 1: Testing modo LOCAL (AI OFF)")
        print("="*80)
        
        if self.toggle_ai_mode(False):
            time.sleep(2)  # Esperar a que el cambio se propague
            self.results['claude_ai_off'] = self.run_full_test_suite("LOCAL")
            self.results['claude_ai_off']['summary'] = self.calculate_summary_stats(
                self.results['claude_ai_off']
            )
        else:
            print("âŒ No se pudo cambiar al modo LOCAL")
            return False
        
        # Test 2: Modo Claude AI ON  
        print("\n" + "="*80)
        print("ğŸ”„ FASE 2: Testing modo CLAUDE AI (AI ON)")
        print("="*80)
        
        if self.toggle_ai_mode(True):
            time.sleep(2)  # Esperar a que el cambio se propague
            self.results['claude_ai_on'] = self.run_full_test_suite("CLAUDE AI")
            self.results['claude_ai_on']['summary'] = self.calculate_summary_stats(
                self.results['claude_ai_on']
            )
        else:
            print("âŒ No se pudo cambiar al modo CLAUDE AI")
            return False
        
        return True
    
    def save_results(self, filename: str = None):
        """Guardar resultados en archivo JSON"""
        if not filename:
            filename = f"performance_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        filepath = os.path.join('/Users/rios/Desktop/LLM-PREMIER/testing', filename)
        
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Resultados guardados en: {filepath}")
        return filepath

def main():
    """FunciÃ³n principal"""
    print("Iniciando tests de rendimiento...")
    
    tester = PerformanceTester()
    
    if tester.run_comparative_tests():
        results_file = tester.save_results()
        
        print("\n" + "="*80)
        print("ğŸ¯ RESUMEN DE RESULTADOS")
        print("="*80)
        
        # Resumen rÃ¡pido
        local_summary = tester.results['claude_ai_off']['summary']
        ai_summary = tester.results['claude_ai_on']['summary']
        
        print(f"ğŸ“Š MODO LOCAL:")
        print(f"   â€¢ Tiempo promedio: {local_summary['overall_avg_time']:.3f}s")
        print(f"   â€¢ Ã‰xito promedio: {local_summary['overall_success_rate']:.1%}")
        print(f"   â€¢ Respuesta mÃ¡s rÃ¡pida: {local_summary['fastest_response']:.3f}s")
        print(f"   â€¢ Respuesta mÃ¡s lenta: {local_summary['slowest_response']:.3f}s")
        
        print(f"\nğŸ¤– MODO CLAUDE AI:")
        print(f"   â€¢ Tiempo promedio: {ai_summary['overall_avg_time']:.3f}s")
        print(f"   â€¢ Ã‰xito promedio: {ai_summary['overall_success_rate']:.1%}")
        print(f"   â€¢ Respuesta mÃ¡s rÃ¡pida: {ai_summary['fastest_response']:.3f}s")
        print(f"   â€¢ Respuesta mÃ¡s lenta: {ai_summary['slowest_response']:.3f}s")
        
        # ComparaciÃ³n
        time_diff = ai_summary['overall_avg_time'] - local_summary['overall_avg_time']
        time_ratio = ai_summary['overall_avg_time'] / local_summary['overall_avg_time'] if local_summary['overall_avg_time'] > 0 else 0
        
        print(f"\nâš–ï¸ COMPARACIÃ“N:")
        print(f"   â€¢ Claude AI es {time_diff:+.3f}s {'mÃ¡s lento' if time_diff > 0 else 'mÃ¡s rÃ¡pido'}")
        print(f"   â€¢ Ratio: Claude AI es {time_ratio:.1f}x el tiempo del modo local")
        
        print(f"\nğŸ“„ Ver reporte completo ejecutando:")
        print(f"   python testing/generate_report.py {results_file}")
        
        return True
    else:
        print("âŒ Error ejecutando tests")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
