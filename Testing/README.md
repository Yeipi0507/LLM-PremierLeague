# ğŸ§ª LLM Premier League - Testing Suite

Sistema completo de testing para evaluar y comparar el rendimiento entre **Claude AI mode** y **Local mode**.

## ğŸ“‹ DescripciÃ³n General

Esta suite de testing evalÃºa:
- **Rendimiento**: Tiempos de respuesta, throughput, confiabilidad
- **Calidad**: PrecisiÃ³n de predicciones, calidad de anÃ¡lisis, conversaciones
- **Carga**: Capacidad bajo mÃºltiples usuarios concurrentes
- **Stress**: LÃ­mites del sistema y puntos de quiebre

## ğŸ—‚ï¸ Estructura de Archivos

```
testing/
â”œâ”€â”€ performance_test.py      # Tests de rendimiento bÃ¡sico
â”œâ”€â”€ quality_test.py          # Tests de calidad de respuestas  
â”œâ”€â”€ load_stress_test.py      # Tests de carga y stress
â”œâ”€â”€ run_all_tests.py         # Master runner - ejecuta todo
â”œâ”€â”€ results/                 # Directorio de resultados
â””â”€â”€ README.md               # Esta documentaciÃ³n
```

## ğŸš€ EjecuciÃ³n RÃ¡pida

### Prerequisitos
1. **Servidor API corriendo**:
   ```bash
   cd /Users/rios/Desktop/LLM-PREMIER
   python LLM/api_server_optimized.py
   ```

2. **Dependencias instaladas**:
   ```bash
   pip install requests statistics concurrent.futures
   ```

### Ejecutar Todo (Recomendado)
```bash
cd testing
python run_all_tests.py
```
**DuraciÃ³n**: 30-45 minutos  
**Output**: Reporte completo + archivos JSON con resultados

### ğŸ’¸ Ejecutar Tests EconÃ³micos (Budget-Friendly)
```bash
cd testing
python budget_test.py
```
**DuraciÃ³n**: ~7 minutos  
**Output**: Insights bÃ¡sicos con <10 requests vs 200+ de la suite completa
**Ideal para**: Desarrollo iterativo, pruebas rÃ¡pidas, wallets limitados

### Ejecutar Tests Individuales

#### ğŸ’¸ Tests EconÃ³micos (Recomendado para desarrollo)

##### Quick Test (3-5 min)
```bash
python quick_test.py
```
- Solo 6 requests totales
- Compara velocidad LOCAL vs CLAUDE AI
- Resultados bÃ¡sicos pero Ãºtiles

##### Mini Quality Test (2 min)
```bash
python mini_quality_test.py
```
- Solo 4 requests totales
- EvaluaciÃ³n bÃ¡sica de calidad
- Perfecto para iteraciÃ³n rÃ¡pida

#### ğŸš€ Tests Completos (Para anÃ¡lisis profundo)

#### 1. Test de Rendimiento (10-15 min)
```bash
python performance_test.py
```
- Compara velocidad LOCAL vs CLAUDE AI
- Mide todas las endpoints
- EstadÃ­sticas detalladas

#### 2. Test de Calidad (5-8 min)
```bash
python quality_test.py
```
- EvalÃºa precisiÃ³n de predicciones
- Calidad de anÃ¡lisis de equipos
- Naturalidad de conversaciones

#### 3. Test de Carga y Stress (15-20 min)
```bash
python load_stress_test.py
```
- Tests con mÃºltiples usuarios concurrentes
- Escalamiento progresivo hasta punto de quiebre
- Capacidad del servidor

## ğŸ“Š InterpretaciÃ³n de Resultados

### Performance Test Results
```json
{
  "claude_ai_off": {
    "summary": {
      "avg_response_time": 0.15,    // Segundos promedio
      "success_rate": 0.98,         // % de requests exitosos
      "requests_per_second": 12.5   // Throughput
    }
  },
  "claude_ai_on": {
    "summary": {
      "avg_response_time": 2.3,     // MÃ¡s lento pero mÃ¡s inteligente
      "success_rate": 0.95,
      "requests_per_second": 3.2
    }
  }
}
```

### Quality Test Results
```json
{
  "claude_ai_off": {
    "summary": {
      "overall_quality_score": 0.65,  // Score 0-1
      "category_scores": {
        "predictions": 0.7,           // Bueno en predicciones
        "analysis": 0.6,              // BÃ¡sico en anÃ¡lisis
        "chat": 0.65                  // Conversacional limitado
      }
    }
  }
}
```

### Load Test Results
```json
{
  "stress_escalation": {
    "20": {
      "success_rate": 0.95,
      "avg_response_time": 1.2
    },
    "40": {
      "success_rate": 0.78,          // Punto de degradaciÃ³n
      "avg_response_time": 3.5
    }
  }
}
```

## ğŸ¯ Casos de Uso

### Desarrollo y OptimizaciÃ³n
```bash
# Test rÃ¡pido despuÃ©s de cambios
python performance_test.py

# Verificar calidad despuÃ©s de ajustes
python quality_test.py
```

### Pre-ProducciÃ³n
```bash
# Suite completa antes de deploy
python run_all_tests.py
```

### Monitoreo Continuo
```bash
# Script automatizable para CI/CD
python performance_test.py --quick-mode
```

## ğŸ“ˆ MÃ©tricas Clave

### Rendimiento
- **Response Time**: < 1s LOCAL, < 5s CLAUDE AI
- **Success Rate**: > 95% ambos modos
- **Throughput**: 10+ RPS LOCAL, 3+ RPS CLAUDE AI

### Calidad
- **Predictions**: Score > 0.7 LOCAL, > 0.8 CLAUDE AI
- **Analysis**: Score > 0.6 LOCAL, > 0.9 CLAUDE AI
- **Chat**: Score > 0.6 LOCAL, > 0.8 CLAUDE AI

### Carga
- **Concurrent Users**: 20+ LOCAL, 15+ CLAUDE AI
- **Breaking Point**: 50+ usuarios LOCAL, 30+ CLAUDE AI
- **Resource Usage**: CPU/Memory monitoring

## ğŸ”§ PersonalizaciÃ³n

### Agregar Nuevos Tests

1. **Crear test personalizado**:
```python
def my_custom_test():
    # Test especÃ­fico para tu caso de uso
    pass
```

2. **Agregar al master runner**:
```python
self.test_suite['custom'] = {
    'script': 'my_test.py',
    'description': 'Mi Test Personalizado',
    'duration_estimate': '5 minutos'
}
```

### Configurar Endpoints de Test
```python
# En cualquier script, modifica:
self.test_endpoints = [
    {'name': 'mi_endpoint', 'method': 'POST', 'path': '/mi-api', 
     'payload': {'param': 'value'}}
]
```

### Ajustar ParÃ¡metros de Carga
```python
self.test_scenarios = {
    'light_load': {'users': 3, 'requests_per_user': 5, 'delay': 1.0},
    'custom_load': {'users': 15, 'requests_per_user': 10, 'delay': 0.3}
}
```

## ğŸš¨ Troubleshooting

### Error: "Servidor API no disponible"
```bash
# Verificar servidor
curl http://localhost:8080/api/health

# Reiniciar servidor
python LLM/api_server_optimized.py
```

### Error: "Timeout exceeded"
- Aumentar timeouts en scripts
- Verificar recursos del sistema
- Reducir carga de test

### Error: "Toggle AI mode failed"
- Verificar archivo `.env` 
- Confirmar API keys vÃ¡lidas
- Revisar logs del servidor

### Resultados Inconsistentes
- Ejecutar mÃºltiples veces
- Verificar carga del sistema
- Cerrar aplicaciones pesadas

## ğŸ“‹ Checklist Pre-Testing

- [ ] Servidor API corriendo en puerto 8080
- [ ] Archivo `.env` configurado correctamente
- [ ] Claude API key vÃ¡lida (si testeas AI mode)
- [ ] Sistema con recursos suficientes
- [ ] Directorio `testing/results/` creado
- [ ] Sin otras cargas pesadas en el sistema

## ğŸ“Š Archivos de Resultados

Todos los resultados se guardan automÃ¡ticamente:

```
testing/results/
â”œâ”€â”€ performance_results_YYYYMMDD_HHMMSS.json
â”œâ”€â”€ quality_results_YYYYMMDD_HHMMSS.json  
â”œâ”€â”€ load_stress_results_YYYYMMDD_HHMMSS.json
â””â”€â”€ master_test_results_YYYYMMDD_HHMMSS.json
```

Cada archivo incluye:
- Timestamp de ejecuciÃ³n
- Resultados detallados por modo
- EstadÃ­sticas calculadas
- Datos raw para anÃ¡lisis posterior

## ğŸ¯ Recomendaciones

### Para Desarrollo
1. Ejecutar `performance_test.py` despuÃ©s de cada cambio significativo
2. Usar `quality_test.py` para validar mejoras en respuestas
3. Ejecutar suite completa antes de commits importantes

### Para ProducciÃ³n
1. Ejecutar suite completa mensualmente
2. Monitorear degradaciÃ³n de rendimiento
3. Establecer alertas basadas en mÃ©tricas clave
4. Usar resultados para planificar scaling

### Para AnÃ¡lisis
1. Comparar resultados histÃ³ricos
2. Identificar patrones de degradaciÃ³n
3. Optimizar basado en bottlenecks encontrados
4. Ajustar configuraciones segÃºn uso real

---

**ğŸ’¡ Tip**: Ejecuta `python run_all_tests.py` para obtener un reporte completo que te ayudarÃ¡ a tomar decisiones informadas sobre quÃ© modo usar en quÃ© situaciones.
